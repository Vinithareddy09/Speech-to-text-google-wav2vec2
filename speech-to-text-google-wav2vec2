import os
import speech_recognition as sr
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer
from scipy.io import wavfile
from scipy.signal import resample
import numpy as np

def transcribe_with_speechrecognition(audio_file):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio = recognizer.record(source)
    try:
        return recognizer.recognize_google(audio)
    except Exception as e:
        return f"SpeechRecognition error: {e}"

def transcribe_with_wav2vec2(audio_file):
    tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
    model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
    sample_rate, data = wavfile.read(audio_file)
    if len(data.shape) > 1:
        data = data[:, 0]
    if data.dtype == np.int16:
        data = data.astype(np.float32) / 32768.0
    elif data.dtype == np.int32:
        data = data.astype(np.float32) / 2147483648.0
    elif data.dtype == np.uint8:
        data = (data.astype(np.float32) - 128) / 128.0
    target_rate = 16000
    if sample_rate != target_rate:
        num_samples = int(len(data) * float(target_rate) / sample_rate)
        data = resample(data, num_samples)
    input_values = tokenizer(data, return_tensors="pt", padding="longest").input_values
    with torch.no_grad():
        logits = model(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    return tokenizer.batch_decode(predicted_ids)[0]

if name == "main":
    audio_file = "demoaudio.wav"
    if not os.path.exists(audio_file):
        print(f"File not found: {audio_file}")
        exit(1)
    print(f"Transcribing file: {audio_file}\n")
    print("--- SpeechRecognition (Google Web Speech API) ---")
    print(transcribe_with_speechrecognition(audio_file))
    print("\n--- Wav2Vec2 (HuggingFace Transformers) ---")
    print(transcribe_with_wav2vec2(audio_file))
